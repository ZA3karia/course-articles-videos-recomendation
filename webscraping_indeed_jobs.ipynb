{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "webscraping_indeed_jobs.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZA3karia/course-articles-videos-recomendation/blob/master/webscraping_indeed_jobs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSUvOr9NwKRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Jul 18 14:47:12 2019\n",
        "\n",
        "@author: ZAZA\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from time import sleep\n",
        "import requests\n",
        "# be kind to the server - set num seconds to delay prior to\n",
        "# fetching each URL\n",
        "FETCH_DELAY_SECONDS = 0.01 # or whatever value you're comfortable with\n",
        "#from newspaper import Article\n",
        "from pandas import DataFrame\n",
        "\n",
        "\n",
        "def get_soup(url):\n",
        "   sleep(FETCH_DELAY_SECONDS)\n",
        "   req = requests.get(url)\n",
        "   soup = BeautifulSoup(req.text, 'html.parser') \n",
        "   return soup\n",
        "\n",
        "\n",
        "\n",
        "def grab_job_links(soup):\n",
        "    \"\"\"\n",
        "    Grab all non-sponsored job posting links from a Indeed search result page using the given soup object\n",
        "    \n",
        "    Parameters:\n",
        "        soup: the soup object corresponding to a search result page\n",
        "                e.g. https://ca.indeed.com/jobs?q=data+scientist&l=Toronto&start=20\n",
        "    \n",
        "    Returns:\n",
        "        urls: a python list of job posting urls\n",
        "    \n",
        "    \"\"\"\n",
        "    i=0\n",
        "    urls = []\n",
        "    for div in soup.find_all('div', {'class': 'jobsearch-SerpJobCard unifiedRow row result'}):\n",
        "      for link in div.find_all('div',{'class': 'title'}):\n",
        "        i+=1\n",
        "        try:\n",
        "          partial_url = link.a.get('href')\n",
        "          url = 'https://www.indeed.com' + partial_url  \n",
        "          if i%2==0:\n",
        "            urls.append(url)\n",
        "            print(i/2)\n",
        "        except:\n",
        "          continue\n",
        "    \n",
        "    return urls\n",
        "\n",
        "\n",
        "\n",
        "def get_urls(query, num_pages, location):\n",
        "    \"\"\"\n",
        "    Get all the job posting URLs resulted from a specific search.\n",
        "    \n",
        "    Parameters:\n",
        "        query: job title to query\n",
        "        num_pages: number of pages needed\n",
        "        location: city to search in\n",
        "    \n",
        "    Returns:\n",
        "        urls: a list of job posting URL's (when num_pages valid)\n",
        "    \"\"\"\n",
        "    # We always need the first page\n",
        "    base_url = 'https://www.indeed.com/jobs?q={}&l={}'.format(query, location)\n",
        "    soup = get_soup(base_url)\n",
        "    urls = []\n",
        "    urls.append( grab_job_links(soup))\n",
        "    # Additional work is needed when more than 1 page is requested\n",
        "    if num_pages >= 2:\n",
        "        # Start loop from page 2 since page 1 has been dealt with above\n",
        "        for i in range(2, num_pages+1):\n",
        "            num = (i-1) * 10\n",
        "            base_url = 'https://www.indeed.com/jobs?q={}&l={}&start={}'.format(query, location, num)\n",
        "            print(base_url)\n",
        "            \n",
        "            soup = get_soup(base_url)\n",
        "            # We always combine the results back to the list\n",
        "            #print(\"current urls state :\" + grab_job_links(soup)[0])\n",
        "            urls.append(grab_job_links(soup))\n",
        "                \n",
        "            \"\"\"\n",
        "            \n",
        "            try:\n",
        "                soup = get_soup(base_url)\n",
        "                # We always combine the results back to the list\n",
        "                print(\"current urls state :\" + grab_job_links(soup))\n",
        "                urls.append(grab_job_links(soup))\n",
        "                \n",
        "            except:\n",
        "                continue\n",
        "            \"\"\"\n",
        "\n",
        "    # Check to ensure the number of urls gotten is correct\n",
        "    #assert len(urls) == num_pages * 10, \"There are missing job links, check code!\"\n",
        "\n",
        "    return urls     \n",
        "\n",
        "\n",
        "\n",
        "def get_posting(url):\n",
        "    \"\"\"\n",
        "    Get the text portion including both title and job description of the job posting from a given url\n",
        "    \n",
        "    Parameters:\n",
        "        url: The job posting link\n",
        "        \n",
        "    Returns:\n",
        "        title: the job title (if \"data scientist\" is in the title)\n",
        "        posting: the job posting content    \n",
        "    \"\"\"\n",
        "    # Get the url content as BS object\n",
        "    soup = get_soup(url)\n",
        "    \n",
        "    # The job title is held in the h3 tag\n",
        "    title = soup.find(name='h3').getText().lower()\n",
        "    posting = soup.find(name='div', attrs={'class': \"jobsearch-jobDescriptionText\"}).get_text()\n",
        "\n",
        "    return title, posting.lower()\n",
        "\n",
        "    #if 'data scientist' in title:  # We'll proceed to grab the job posting text if the title is correct\n",
        "        # All the text info is contained in the div element with the below class, extract the text.\n",
        "        #posting = soup.find(name='div', attrs={'class': \"jobsearch-JobComponent\"}).get_text()\n",
        "        #return title, posting.lower()\n",
        "    #else:\n",
        "        #return False\n",
        "    \n",
        "        # Get rid of numbers and symbols other than given\n",
        "        #text = re.sub(\"[^a-zA-Z'+#&]\", \" \", text)\n",
        "        # Convert to lower case and split to list and then set\n",
        "        #text = text.lower().strip()\n",
        "    \n",
        "        #return text\n",
        "      \n",
        "\"\"\"\n",
        "def get_art(url):\n",
        "  article = Article(url)\n",
        "  article.download()\n",
        "  article.parse()\n",
        "  article.nlp()\n",
        "  title = article.title\n",
        "  text = article.text\n",
        "  return title,text\n",
        "\n",
        "\"\"\"\n",
        "def get_data(query, num_pages, location='usa'):\n",
        "    \"\"\"\n",
        "    Get all the job posting data and save in a json file using below structure:\n",
        "    \n",
        "    {<count>: {'title': ..., 'posting':..., 'url':...}...}\n",
        "    \n",
        "    The json file name has this format: \"\"<query>.json\"\n",
        "    \n",
        "    Parameters:\n",
        "        query: Indeed query keyword such as 'Data Scientist'\n",
        "        num_pages: Number of search results needed\n",
        "        location: location to search for\n",
        "    \n",
        "    Returns:\n",
        "        postings_dict: Python dict including all posting data\n",
        "    \n",
        "    \"\"\"\n",
        "    # Convert the queried title to Indeed format\n",
        "    query = '+'.join(query.lower().split())\n",
        "    \n",
        "    #postings_dict = {}\n",
        "    tab_url = []\n",
        "    tab_titles = []\n",
        "    tab_post = []\n",
        "    urls = get_urls(query, num_pages, location)\n",
        "    print(\"---------------------------------------------part1-------------------------------------------------\")\n",
        "    #  Continue only if the requested number of pages is valid (when invalid, a number is returned instead of list)\n",
        "    i=0\n",
        "    print(\"----------------------------urls:\" )\n",
        "    print(urls)\n",
        "    for page_urls in urls:\n",
        "      \n",
        "      print(\"page urls :\" )\n",
        "      print(page_urls)\n",
        "      for url in page_urls:\n",
        "        print(\"url\")\n",
        "        print( url)\n",
        "        try:\n",
        "            title, posting = get_posting(url)\n",
        "            #title, posting = get_art(url)\n",
        "            tab_url.append(url)\n",
        "            tab_titles.append(title)\n",
        "            tab_post.append(posting)\n",
        "            #postings_dict[i] = {}\n",
        "            #postings_dict[i]['title'], postings_dict[i]['posting'], postings_dict[i]['url'] = title, posting, url\n",
        "            postings_table.append(post)\n",
        "            #i+=1\n",
        "        except: \n",
        "            continue\n",
        "        \"\"\"\n",
        "        \n",
        "        percent = (i+1) / num_urls\n",
        "        # Print the progress the \"end\" arg keeps the message in the same line \n",
        "        print(\"Progress: {:2.0f}%\".format(100*percent), end='\\r')\n",
        "\n",
        "        \"\"\"\n",
        "    posting_dict = {'title' : tab_titles,\n",
        "                    'url' : tab_url,\n",
        "                    'post' : tab_post\n",
        "    }\n",
        "    # Save the dict as json file\n",
        "    file_name = query.replace('+', '_') + '.json'\n",
        "    with open(file_name, 'w') as f:\n",
        "        json.dump(posting_dict, f)\n",
        "\n",
        "    print('All {} postings have been scraped and saved!'.format(i))    \n",
        "    \n",
        "    df = DataFrame(posting_dict, columns = ['title','url','post'])\n",
        "    file_name = query.replace('+', '_') + '.csv'\n",
        "    df.to_csv( file_name)\n",
        "    return posting_dict,df\n",
        "\n",
        "\n",
        "\n",
        "# If script is run directly, we'll take input from the user\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    query = input(\"Please enter the title to scrape data for: \\n\").lower()\n",
        "    \n",
        "    while True:\n",
        "        num_pages = input(\"Please enter the number of pages needed (integer only): \\n\")\n",
        "        try:\n",
        "            num_pages = int(num_pages)\n",
        "            break\n",
        "        except:\n",
        "            print(\"Invalid number of pages! Please try again.\")\n",
        "    get_data(query, num_pages, location='usa')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}